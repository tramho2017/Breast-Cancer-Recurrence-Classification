---
title: "5790 Project"
output: html_document
date: "2023-08-24"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

library(e1071)
library(caret)
library(recipes)
library(VIM)
library(MLmetrics)
library(pROC)
library(mda)
library(kernlab)
library(pamr)
library(klaR)
```

# Reading Data
```{r}
data<-read.csv("breast-cancer-data.csv")

# Creating dummy Variables
simple<-dummyVars(class~age+menopause+tumer.size+inv.nodes+node.caps+deg.malig+breast+breast.quad+irradiate,data=data)
data1<-predict(simple,data)
dim(data1) #286  43
unique(data$class)
y<-c()
y[which(data$class=="recurrence-events'") ]=1
y[which(data$class=="no-recurrence-events'") ]=0
```

#Analysing and graph data
```{r}
# Barplots of categorical predictors
names<-colnames(data1)
par(mfrow=c(5,4))
for (i in 1:20){
counts<-table(data1[,i])
pl<-barplot(counts,main=names[i])
}

par(mfrow=c(6,4))
for (i in 21:43){
counts<-table(data1[,i])
pl<-barplot(counts,main=names[i])
}

# Barplot of response
tableY<-table(y)
barplot(tableY, main="The Distribution of Response Variable")
```

#Preprocessing Data
```{r}
heatmap(cor(data1), main="Heatmap of Variable Correlations \n before Transformation")
apply(data1,2 ,skewness)
n<-nearZeroVar(data1,freqCut=95/5)
length(n)
data2<-data1[,-n]
dim(data2)
trans1<-kNN(data2,imp_var=FALSE)
corre<-cor(trans1)
heatmap(corre, main="Heatmap of Variable  Correlations")
t<-findCorrelation(corre,cutoff=0.8)
length(t)
data3<-trans1[,-t]
apply(data3,2 ,skewness)
trans<-preProcess(data3,method=c("BoxCox","center","scale"))
data4<-predict(trans,data3)
apply(data4,2 ,skewness)
data5<-spatialSign(data4)
apply(data5,2 ,skewness)
heatmap(cor(data5), main="Heatmap of Correlations \n after Transformation")
```

#Splititng Data
```{r,echo=FALSE}
y1<-c()
y1[which(y==0)]<-"Class1"
y1[which(y==1)]<-"Class2"

unique(y1)
y2<-factor(y1,labels=c('Class1','Class2'))
set.seed(123)
index<-createDataPartition(y,p=0.8,list=FALSE)
trainX<-data5[index,]
trainY<-y2[index]
testX<-data5[-index,]
testY<-y2[-index]
length(trainY) #229
length(testY) # 57
dim(trainX) #229  26
dim(testX) #57 26
```

# Buiding Model
# Logistic Regression
```{r}
set.seed(1)
ctrl <- trainControl(method = "LGOCV",
summaryFunction = twoClassSummary,
classProbs = TRUE,
##index = list(simulatedTest[,1:4]),
savePredictions = TRUE)
lModel <- train(trainX,
y = trainY,
method = "glm",
metric = "auc",
# preProc=c("center","scale","pca")
trControl = ctrl)

#Predict on the training set
LPred<-predict(lModel, trainX)
postResample(pred=LPred,obs=trainY)
confusionMatrix(lModel$pred$pred,lModel$pred$obs)
LPredProbs<-predict(lModel, newdata=trainX,type="prob")
LFullRoc<-multiclass.roc(response=trainY, predictor=LPredProbs)
auc(LFullRoc)# 0.8149

```

##LDA 
```{r,echo=FALSE}
ctrl <- trainControl(method = "LGOCV",
                     summaryFunction =multiClassSummary,
                     classProbs = TRUE,
                     ##index = list(simulatedTest[,1:4]),
                    # preprocess("")
                     savePredictions = TRUE)
LDATrain <- train(x=trainX,trainY,method = "lda", metric = "auc", trControl = ctrl)

#Predict on the training set
LPred<-predict(LDATrain, trainX)
postResample(pred=LPred,obs=trainY)
confusionMatrix(LDATrain$pred$pred,LDATrain$pred$obs)
LPredProbs<-predict(LDATrain, newdata=trainX,type="prob")
FullRoc<-multiclass.roc(response=trainY, predictor=LPredProbs)
auc(FullRoc)

```
## PLSDA 
```{r,echo=FALSE}
set.seed(1)
ctrl <- trainControl(method = "LGOCV",summaryFunction =twoClassSummary,classProbs = TRUE,savePredictions = TRUE)
plsModel <- train(x = trainX,
y = trainY,
method = "pls",
tuneGrid = expand.grid(.ncomp = 1:26),
preProc = c("center","scale"),
metric = "auc",
trControl = ctrl)
plot(plsModel,main="ROC of  PLSDA Model")
plsPredProbs<-predict(plsModel, newdata=trainX,type="prob")
plsRoc<-multiclass.roc(response=trainY, predictor=plsPredProbs)
auc(plsRoc) 
```

#Penalized Model
```{r}
ctrl <- trainControl(method = "LGOCV",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE,
                     ##index = list(simulatedTest[,1:4]),
                     savePredictions = TRUE)

glGrid <- expand.grid(.alpha = c(0, .1, .2, .4, .6, .8, 1),
                        .lambda = seq(.01, .2, length = 40))
set.seed(476)
glTune<- train(x=trainX,
                   y = trainY,
                   method = "glmnet",
                   tuneGrid = glGrid,
                   preProc = c("center", "scale"),
                   metric = "auc",
                   trControl = ctrl)
glTune
plot(glTune,main="Tuning Parameters of Penalized Model")


#Predict on the training set:
gPred<-predict(glTune, trainX)
postResample(pred=gPred,obs=trainY)
confusionMatrix(glTune$pred$pred,glTune$pred$obs)
gPredProbs<-predict(glTune, newdata=trainX,type="prob")
gRoc<-multiclass.roc(response=trainY, predictor=gPredProbs)
auc(gRoc)
```

# SVM
```{r}

ctrl <- trainControl(summaryFunction = twoClassSummary,
                     classProbs = TRUE)
sigma <- sigest(as.matrix(trainX))
svmRGrid <- expand.grid(.sigma = sigma[1],
                                 .C = 2^(seq(-2, 7)))

svmModel <- train(x = trainX, 
                   y = trainY,
                   method = "svmRadial",
                   metric = "auc",
                   preProc = c("center", "scale"),
                   tuneGrid = svmRGrid,
                   fit = FALSE,
                   trControl = ctrl)
svmModel
summary(svmModel)
plot(svmModel, main=" Tuning Parameters of\n Support Vector Machine Model ")

#Predict on the training set:
sPred <- predict(svmModel, newdata = trainX,type="raw")

confusionMatrix(data = sPred,reference =trainY)
sPredProbs<-predict(svmModel, newdata=trainX,type="prob")
sRoc<-multiclass.roc(response=trainY, predictor=sPredProbs)
auc(sRoc) #0.819

#Predict on the test set:
svmPred <- predict(svmModel, newdata = testX,type="raw")
confusionMatrix(data = svmPred,reference =testY)
svmPredProbs<-predict(svmModel, newdata=testX,type="prob")
svmRoc<-multiclass.roc(response=testY, predictor=svmPredProbs)
auc(svmRoc) #0.7151

#Important Variable
varImp(svmModel)
```


#Flexible Discriminant Analysis
```{r}
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:30)
fdaTuned <- train(x = trainX, 
                   y = trainY,
                   method = "fda",
                   preProcess = c("center", "scale"),
                   # Explicitly declare the candidate models to test
                   tuneGrid = marsGrid,
                   trControl = trainControl(method = "cv"))
fdaTuned
plot(fdaTuned, main="Tuning Parameters of \n Flexible Discriminant Analysis Model ")

#Predict on the training set:
fPred <- predict(fdaTuned, newdata = trainX,type="raw")
confusionMatrix(data = fPred,reference =trainY)
fPredProbs<-predict(fdaTuned, newdata=trainX,type="prob")
fRoc<-multiclass.roc(response=trainY, predictor=fPredProbs)
auc(fRoc) #   0.7755

```

#Neural Networks
```{r}
library(caret)
Grid <- expand.grid(.size = 1:10, .decay = c(0, .1, .3, .5, 1))
maxSize <- max(Grid$.size)
dim(trainX) #229  26
numWts <- (maxSize * (26 + 1) + (maxSize+1)*2) ## 26 is the number of predictors; 2 is the number of classes
set.seed(476)
ctrl <- trainControl(summaryFunction = twoClassSummary,
                     classProbs = TRUE)

nnetFit <- train(x = trainX, 
                 y = trainY,
                 method = "nnet",
                 metric = "auc",
                 preProc = c("center", "scale", "spatialSign"),
                 tuneGrid = Grid,
                 trace = FALSE,
                 maxit = 2000,
                 MaxNWts = numWts,
                 trControl = ctrl)
nnetFit
plot(nnetFit,main="Tuning Parameters of \n Neural Networks Model")

#Predict on the training set:
nPred<-predict(nnetFit, trainX)
confusionMatrix(nPred,trainY)
nPredProbs<-predict(nnetFit , newdata=trainX,type="prob")
nRoc<-multiclass.roc(response=trainY, predictor=nPredProbs)
auc(nRoc)#0.7997

```

#KNN
```{r}
set.seed(123)
ctrl<-trainControl(summaryFunction=twoClassSummary, classProbs=TRUE)
knnTune <- train(x = trainX, 
                y = trainY,
                method = "knn",
                metric = "auc",
                preProc = c("center", "scale"),
                ##tuneGrid = data.frame(.k = c(4*(0:5)+1, 20*(1:5)+1, 50*(2:9)+1)), ## 21 is the best
                tuneGrid = data.frame(.k = 1:50),
                trControl = ctrl)

knnTune
plot(knnTune,main="Tuning Parameters of KNN Model")

#Predict on the training set:
knnTune$results$Accuracy
kPred<-predict(knnTune, trainX)
postResample(pred=kPred,obs=trainY)
confusionMatrix(kPred,trainY)
kPredProbs<-predict(knnTune, newdata=trainX,type="prob")
kRoc<-multiclass.roc(response=trainY, predictor=kPredProbs)
auc(kRoc) # 0.7581

```

# Naive Bayes 
```{r}
set.seed(123)
nbTune <- train( x = trainX, 
                y = trainY,
                method = "naive_bayes",
                metric = "ROC",
                preProc = c("center", "scale"),
                #tuneGrid =expand.grid(usekernel=TRUE, adjust=1,fL=c(0.2,0.5,0.8)),
               # tuneGrid=data.frame(.fL = 2,.adjust=1,.usekernel = TRUE,.adjust = TRUE),
                tuneGrid=data.frame(.laplace =c(0,0.5,1.0,2.0) ,.adjust=c(0,0.5,1.0,2.0),.usekernel = TRUE),
               trControl = trainControl(method="cv",number=5,classProbs=TRUE,))

nbTune
plot.new()
plot(nbTune,main="Tuning Parameters of \n Naive Bayes Model")

#Predict on the training set:
nPred<-predict(nbTune, trainX)
postResample(pred=nPred,obs=trainY)
confusionMatrix(nPred,trainY)
nPredProbs<-predict(nbTune, newdata=trainX,type="prob")
nRoc<-multiclass.roc(response=trainY, predictor=nPredProbs)
auc(nRoc) # 0.8285

#Predict on the test set:
nbPred<-predict(nbTune, testX)
postResample(pred=nbPred,obs=testY)
confusionMatrix(nbPred,testY)
nbPredProbs<-predict(nbTune, newdata=testX,type="prob")
nbRoc<-multiclass.roc(response=testY, predictor=nbPredProbs)
auc(nbRoc) #

```



